<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on July 28, 2020</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT03764761</url>
  </required_header>
  <id_info>
    <org_study_id>Storybook</org_study_id>
    <secondary_id>R01HD083381-01A1</secondary_id>
    <nct_id>NCT03764761</nct_id>
  </id_info>
  <brief_title>Storybook Reading in Individuals With Down Syndrome</brief_title>
  <official_title>Eye Tracking Technologies to Characterize and Optimize Visual Attending in Down Syndrome</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>Penn State University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
  </sponsors>
  <source>Penn State University</source>
  <oversight_info>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>Yes</is_fda_regulated_device>
    <is_us_export>No</is_us_export>
  </oversight_info>
  <brief_summary>
    <textblock>
      This study uses mobile eye-tracking technology in order to characterize patterns of visual
      attention to communication supports, as well as a partner, within real world interactions for
      individuals with Down syndrome.

      Visual communication supports are central components of what is termed augmentative and
      alternative communication (AAC) intervention. AAC refers to the methods and technology
      designed to supplement spoken communication for people with limited speech. &quot;Aided&quot; AAC is a
      subcategory in which an external aid stores and presents for use visual symbols such as
      photographs, line drawings, or alphabet letters. The most traditional means of structuring
      aided AAC displays is to present the language concepts within row-column grids, which contain
      individual symbols/concepts placed in each grid square. The investigator's previous work
      investigated whether these grid-based presentations could be improved by understanding how
      different perceptual features of the displays influence responding (i.e., whether what the
      display looks like influences how easily the information on it is found). Individuals with
      developmental disabilities and children developing typically were faster and more accurate in
      finding information on some displays over others, when tested using a &quot;visual search&quot; task
      (aka, a &quot;finding game&quot; - &quot;find the dog&quot;).

      The previous investigations have evaluated visual attention within a setting that isolated
      visual processing of the AAC display as the primary dependent measure. However, communication
      requires attention not only to an AAC display, but also to a communication partner.
      Therefore, the current study seeks to examine questions of visual attention to both an AAC
      display and a communication partner. The investigators will manipulate characteristics of the
      structure of the display (e.g., arrangement of symbols), in order to determine if more
      optimal displays facilitate desirable patterns of visual attention to both the communication
      display and the partner. The mobile eye-tracking technology captures attention to both the
      display and the communication partner. The investigators anticipate that participants will be
      able to attend to their partner and the shared activity more when the AAC display is more
      optimal, but that when the AAC display is sub-optimal, the participants will have to spend
      more time examining the AAC display and less time in actual communication.
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Visual supports are central components of what is termed augmentative and alternative
      communication (AAC) intervention within speech-language pathology. AAC refers to the methods
      and technology designed to supplement spoken communication for people with limited speech.
      &quot;Aided&quot; AAC is a subcategory in which an external aid stores and presents for use visual
      symbols such as photographs, line drawings, or alphabet letters. Aided AAC relies on vision
      for access. If users cannot fully attend to, understand, or process the semantic information
      on a visual display, they are unlikely to use that display effectively. Regrettably, little
      research has focused on AAC display design variables that enhance attention

      This research seeks to gain a greater understanding of visual attention to AAC displays and
      communication partners in order to further optimize display design. Eye tracking technology
      will reveal attention patterns that typically go unrecorded in behavioral research,
      particularly in individuals with severe disabilities. Specifically, eye tracking technology
      permits recording of the coordinates of where the participant is looking at any given time,
      how long they look, and what they ignore. This study seeks to record eye gaze via eye
      tracking during a shared book reading activity in which the AAC display is used for
      communication with a partner. It will help to determine whether optimal displays, which
      facilitate speed to locate targets and minimize fixations to distractors, will promote
      attention to the partner. Ultimately, this information will contribute to improving the
      design of materials for children with disabilities who require AAC.

      The most traditional means of structuring aided AAC displays is to present the language
      concepts within traditional row-column grids, which contain individual symbols/concepts
      placed in each grid square. The investigator's earlier work examined whether these grid-based
      presentations could be improved by understanding how different perceptual features of the
      displays influence responding (ie, whether what the display looks like influences how easily
      the information on it is found). Individuals with developmental disabilities and children
      developing typically were faster and more accurate in finding information on some displays
      over others, when tested using a &quot;visual search&quot; task (aka, a &quot;finding game&quot; - &quot;find the
      dog&quot;). The next study then examined the reason behind this phenomenon by using eye tracking
      technology to examine how visual search itself was influenced by the different displays.
      Results indicated that in individuals with and without disabilities, in the non-optimal
      display there were significantly more fixations (looks) to non-relevant distractors than on
      the optimal display. Given that individuals with disabilities, including Down syndrome, are
      prone to ready distraction, the use of a display that by its very structure promotes looks to
      distractors seems to be a potentially critical mistake.

      The current study examines the effects of adding a communication partner on the allocation of
      visual attention to optimally and non-optimally designed displays. This study is a
      translational step of moving from more basic research towards more clinically relevant
      research. Of interest are two questions: (1) How is a social partner integrated into the
      attentional field of the individual using AAC, in general, and (2) What is the effect of the
      introduction of the partner/social communication task on the patterns of attention across
      different display conditions?

      Participants who have participated in the earlier research of the PI will be contacted to see
      if they would like to return for this one. Participants who express interest in learning more
      will be sent the phone/email information, the recruitment flyer and, if they request it, the
      consent form. If after reading these the participants are still interested, scheduling will
      begin.

      First, during the assessment, participants will be assessed with the Peabody Picture
      Vocabulary Test - Fourth Edition (PPVT-4), which is an assessment of receptive vocabulary
      skills. It is the gold standard in both speech and language assessments as well as research,
      for estimating vocabulary size. In this, the child is shown four pictures at a time, and
      asked to choose one of them on the basis of the spoken word. The test continues until the
      child makes more than 8 errors in a set of 12. The test takes generally about 20 minutes to
      complete.

      After the assessment portion is completed, participants will return for up to five additional
      sessions to undergo the storybook reading portion of the study. Each visit will involve
      reading two separate books with a trained research assistant and should last about 30
      minutes. Before reading the books, the research assistant will conduct a preference
      assessment during which the participant will be given a choice of 4-6 possible sets of books
      that he/she will read over the length of the study. The participant will be provided with
      pictures of choices and can indicate their preferred choice by speaking, pointing to, or
      selecting their choice.

      While participants are engaged in book-reading, they will also be wearing Tobii Pro eye
      tracking glasses that have an eye tracking device embedded within. It is ultra-lightweight
      glasses with a highly unobtrusive head unit. Mobile eye tracking goggles allow for recording
      of gaze path directly within the frames of glasses (similar to Google glasses). These mobile
      technologies enable visualization and analysis of allocation of visual attention during live
      social interactions, as the recording apparatus moves simultaneously with the movement of the
      participant's head and records the changing field of vision. The technology uses extremely
      low-level infrared light that is bounced off the pupil of the participant. The amount of
      infrared light is smaller than that found in the typical television remote and is far below
      federal safety requirements. The glasses include a non-invasive strap that will be tightened
      at the back to ensure the glasses stay in place when they are worn.

      Prior to placing the glasses on the participant, the research assistant will follow a
      protocol to allow the participant to become familiar with the eye-tracking glasses. This will
      involve watching a short video that shows another person wearing the glasses. Then, the
      participant will be invited to place a pair of sunglasses on that has a strap similar to the
      eye-tracking goggles. The research assistant will tighten the strap and allow the child to
      wear the sunglasses for several minutes to become accustomed to the strap. Next, the
      participant will be fitted with the eye-tracking glasses. If the child wears prescription
      eye-glasses, the lenses in the glasses will be changed to match their prescription (there are
      multiple lenses that can be switched in or out of the glasses themselves), which will be
      obtained from the parents on the demographic form.

      One the child is fitted with the eye-tracking glasses, the book reading will begin. The
      research assistant will read a book to the participant. The participant will be positioned in
      front of an AAC display that include symbols/messages to comment about the book. Each
      participant will undergo several sessions of book reading. The participant will interact with
      the partner during the book reading exchange by using an AAC display that contains symbols to
      comment about the book. The participant may use the AAC display by accessing it with a mouse
      or directly touching the symbols. The research assistant will follow a script that includes
      different types of questions directed towards the participant.

      In between the two books read each session, the participant will be offered a snack, that is
      a snack approved by the parents/guardians prior to participation.

      The sessions will be video recorded using a video camera. This will allow for post hoc review
      to ensure the fidelity of adherence to the script by the trained research assistant.
    </textblock>
  </detailed_description>
  <overall_status>Recruiting</overall_status>
  <start_date type="Actual">April 1, 2018</start_date>
  <completion_date type="Anticipated">November 30, 2020</completion_date>
  <primary_completion_date type="Anticipated">November 30, 2020</primary_completion_date>
  <phase>N/A</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>N/A</allocation>
    <intervention_model>Single Group Assignment</intervention_model>
    <intervention_model_description>Within subjects alternating treatment design</intervention_model_description>
    <primary_purpose>Device Feasibility</primary_purpose>
    <masking>None (Open Label)</masking>
  </study_design_info>
  <primary_outcome>
    <measure>Percent of visual fixation time on meaningful and non meaningful stimuli</measure>
    <time_frame>1-6 hours</time_frame>
    <description>Measured through percent of fixation time allocated to (a) the AAC display; (b) the storybook, or (c) the communication partner. Percent is calculated by dividing the total number of samples within each area (AAC display, storybook, partner) into the total number of samples obtained by the eye tracking device.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Number of times the participant communicates during the intervention</measure>
    <time_frame>1-6 hours</time_frame>
    <description>Rate of communication attempts during the storybook reading is defined as the number of times the participant attempts to communicate, divided by the total session duration.</description>
  </primary_outcome>
  <number_of_arms>1</number_of_arms>
  <enrollment type="Anticipated">15</enrollment>
  <condition>Down Syndrome</condition>
  <condition>Augmentative and Alternative Communication</condition>
  <arm_group>
    <arm_group_label>AAC Intervention</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Participants will use AAC technology of different designs delivered on iMacs or Surface tablets</description>
  </arm_group>
  <intervention>
    <intervention_type>Device</intervention_type>
    <intervention_name>AAC Technology - Standard of Care</intervention_name>
    <description>Story Book is separate from AAC symbols, AAC symbols are arranged on a grid with color backgrounds. This is non-optimal arrangement and non-integrated presentation</description>
    <arm_group_label>AAC Intervention</arm_group_label>
    <other_name>Standard of Care</other_name>
  </intervention>
  <intervention>
    <intervention_type>Device</intervention_type>
    <intervention_name>AAC Technology - non-optimal integrated arrangement</intervention_name>
    <description>Story Book is integrated on to the AAC display together with the AAC symbols, AAC symbols are arranged on a grid with color backgrounds. This is non-optimal arrangement and but integrated presentation</description>
    <arm_group_label>AAC Intervention</arm_group_label>
    <other_name>Non-optimal, integrated</other_name>
  </intervention>
  <intervention>
    <intervention_type>Device</intervention_type>
    <intervention_name>AAC Technology - non-optimal integrated arrangement</intervention_name>
    <description>Story Book is integrated on to the AAC display together with the AAC symbols, AAC symbols are arranged on a grid with color backgrounds. This is optimal arrangement and but integrated presentation</description>
    <arm_group_label>AAC Intervention</arm_group_label>
    <other_name>Optimal, integrated</other_name>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:

          -  Participants with Down syndrome who have receptive language age estimates between 3;0
             -7;0 years as measures on the Peabody Picture Vocabulary Test- 4th Edition (PPVT-IV;
             Dunn &amp; Dunn, 2006) and chronological ages of 7 to 35 years.

        Exclusion Criteria:

          -  We will exclude anyone outside the range of 7-35 years, inclusive. We will plan to
             exclude those having: (1) uncontrolled seizures; (2) sensory or peripheral impairment
             that might impair performance; (3) co-morbid illnesses with implications for central
             nervous system function.
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>7 Years</minimum_age>
    <maximum_age>35 Years</maximum_age>
    <healthy_volunteers>No</healthy_volunteers>
  </eligibility>
  <overall_official>
    <last_name>Krista Wilkinson, PhD</last_name>
    <role>Principal Investigator</role>
    <affiliation>Penn State</affiliation>
  </overall_official>
  <overall_contact>
    <last_name>Krista M Wilkinson, PhD</last_name>
    <phone>814-863-2206</phone>
    <email>kmw22@psu.edu</email>
  </overall_contact>
  <location>
    <facility>
      <name>11 Ford Building</name>
      <address>
        <city>University Park</city>
        <state>Pennsylvania</state>
        <zip>16802</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Krista M Wilkinson, PhD</last_name>
      <phone>814-863-2206</phone>
      <email>kmw22@psu.edu</email>
    </contact>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>March 2020</verification_date>
  <study_first_submitted>March 16, 2018</study_first_submitted>
  <study_first_submitted_qc>December 3, 2018</study_first_submitted_qc>
  <study_first_posted type="Actual">December 5, 2018</study_first_posted>
  <last_update_submitted>March 2, 2020</last_update_submitted>
  <last_update_submitted_qc>March 2, 2020</last_update_submitted_qc>
  <last_update_posted type="Actual">March 3, 2020</last_update_posted>
  <responsible_party>
    <responsible_party_type>Principal Investigator</responsible_party_type>
    <investigator_affiliation>Penn State University</investigator_affiliation>
    <investigator_full_name>Krista Wilkinson</investigator_full_name>
    <investigator_title>Professor</investigator_title>
  </responsible_party>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Down Syndrome</mesh_term>
    <mesh_term>Syndrome</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>No</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>

