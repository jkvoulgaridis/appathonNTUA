<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on July 28, 2020</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT04474691</url>
  </required_header>
  <id_info>
    <org_study_id>staRt single-case</org_study_id>
    <secondary_id>R41DC016778</secondary_id>
    <nct_id>NCT04474691</nct_id>
  </id_info>
  <brief_title>staRt: Enhancing Speech Treatment With Smartphone-delivered Biofeedback</brief_title>
  <acronym>staRt</acronym>
  <official_title>staRt: Enhancing Speech Treatment With Smartphone-delivered Biofeedback</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>New York University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>National Institutes of Health (NIH)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
    <collaborator>
      <agency>National Institute on Deafness and Other Communication Disorders (NIDCD)</agency>
      <agency_class>NIH</agency_class>
    </collaborator>
  </sponsors>
  <source>New York University</source>
  <oversight_info>
    <has_dmc>No</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      Previous research suggests that biofeedback can outperform traditional interventions for RSE,
      but no controlled studies have tested this hypothesis in the context of app-delivered
      biofeedback. The objective of this aim is to use the staRt app to test our working hypothesis
      that speakers will make larger gains in /r/ accuracy when app-based treatment incorporates
      biofeedback, compared to a non-biofeedback condition. With a network of cooperating SLPs,
      this project will recruit 15 children with /r/ misarticulation to receive 8 weeks of
      intervention using staRt. Individual sessions will be randomly assigned to include or exclude
      the visual biofeedback display. Randomization tests will be used to evaluate, for each
      individual, whether larger increments of change are associated with biofeedback and
      non-biofeedback sessions.
    </textblock>
  </brief_summary>
  <overall_status>Completed</overall_status>
  <start_date type="Actual">April 4, 2018</start_date>
  <completion_date type="Actual">February 29, 2020</completion_date>
  <primary_completion_date type="Actual">August 31, 2019</primary_completion_date>
  <phase>Phase 1</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>Randomized</allocation>
    <intervention_model>Factorial Assignment</intervention_model>
    <intervention_model_description>Children with /r/ misarticulation to receive 8 weeks of intervention using staRt. Individual sessions will be randomly assigned to include or exclude the visual biofeedback display. Randomization tests will be used to evaluate, for each individual, whether larger increments of change are associated with biofeedback and non-biofeedback sessions.</intervention_model_description>
    <primary_purpose>Treatment</primary_purpose>
    <masking>Single (Outcomes Assessor)</masking>
    <masking_description>Acoustic measures will be obtained by research assistants blinded to the treatment condition assigned for each session.</masking_description>
  </study_design_info>
  <primary_outcome>
    <measure>Acoustically measured accuracy of /r/ in words</measure>
    <time_frame>Immediately before each of 12 treatment sessions</time_frame>
    <description>Probes of words and syllables containing /r/ in various phonetic contexts were elicited at the start and end of treatment. We follow a semi-automated protocol to demarcate intervals of interest in a Praat textgrid. Textgrids and recordings are submitted to a forced aligner, followed by automated extraction of formant frequencies (F1, F2, F3) at the center of each /r/ interval identified. F3-F2 distance, which is smaller in correct /r/, serves as the primary acoustic correlate of accuracy.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Acoustically measured accuracy of /r/ in words</measure>
    <time_frame>Immediately after each of 12 treatment sessions</time_frame>
    <description>Probes of words and syllables containing /r/ in various phonetic contexts were elicited at the start and end of treatment. We follow a semi-automated protocol to demarcate intervals of interest in a Praat textgrid. Textgrids and recordings are submitted to a forced aligner, followed by automated extraction of formant frequencies (F1, F2, F3) at the center of each /r/ interval identified. F3-F2 distance, which is smaller in correct /r/, serves as the primary acoustic correlate of accuracy.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Acoustically measured accuracy of /r/ in words</measure>
    <time_frame>In a separate session within one week of the start of all treatment</time_frame>
    <description>Probes of words and syllables containing /r/ in various phonetic contexts were elicited at the start and end of treatment. We follow a semi-automated protocol to demarcate intervals of interest in a Praat textgrid. Textgrids and recordings are submitted to a forced aligner, followed by automated extraction of formant frequencies (F1, F2, F3) at the center of each /r/ interval identified. F3-F2 distance, which is smaller in correct /r/, serves as the primary acoustic correlate of accuracy.</description>
  </primary_outcome>
  <primary_outcome>
    <measure>Acoustically measured accuracy of /r/ in words</measure>
    <time_frame>In a separate session within one week of the end of all treatment</time_frame>
    <description>Probes of words and syllables containing /r/ in various phonetic contexts were elicited at the start and end of treatment. We follow a semi-automated protocol to demarcate intervals of interest in a Praat textgrid. Textgrids and recordings are submitted to a forced aligner, followed by automated extraction of formant frequencies (F1, F2, F3) at the center of each /r/ interval identified. F3-F2 distance, which is smaller in correct /r/, serves as the primary acoustic correlate of accuracy.</description>
  </primary_outcome>
  <number_of_arms>2</number_of_arms>
  <enrollment type="Actual">15</enrollment>
  <condition>Speech Sound Disorder</condition>
  <arm_group>
    <arm_group_label>Visual-acoustic biofeedback</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
  </arm_group>
  <arm_group>
    <arm_group_label>Traditional articulation treatment</arm_group_label>
    <arm_group_type>Active Comparator</arm_group_type>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Traditional articulation treatment</intervention_name>
    <description>Traditional articulation treatment involves providing auditory models and verbal descriptions of correct articulator placement, then cueing repetitive motor practice. Images and diagrams of the vocal tract can be used as visual aids; however, no real-time visual display of articulatory or acoustic information will be made available. Knowledge of performance feedback could describe either the desired articulator placement or the auditory quality of the target sound.</description>
    <arm_group_label>Traditional articulation treatment</arm_group_label>
  </intervention>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Visual-acoustic biofeedback</intervention_name>
    <description>In visual-acoustic biofeedback treatment, the elements of traditional treatment (auditory models and verbal descriptions of articulator placement) are enhanced with a dynamic display of the speech signal in the form of the real-time LPC (Linear Predictive Coding) spectrum generated by the staRt app. Because correct vs incorrect productions of /r/ contrast acoustically in the frequency of the third formant (F3), participants will be cued to make their real-time LPC spectrum match a visual target characterized by a low F3 frequency. They will be encouraged to attend to the visual display while adjusting the placement of their articulators and observing how those adjustments impact F3. Knowledge of performance feedback will typically involve reference to the location of the third peak or `bump' on the visual display.</description>
    <arm_group_label>Visual-acoustic biofeedback</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:

          -  Normal performance on a pure-tone hearing screening at 20 dB HL, a screening
             examination of oral-motor structure and function, and a test of receptive language.

        Exclusion Criteria:

          -  History of major behavioral, neurological, or hearing impairment, per parent and/or
             SLP report.
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>8 Years</minimum_age>
    <maximum_age>15 Years</maximum_age>
    <healthy_volunteers>No</healthy_volunteers>
  </eligibility>
  <location>
    <facility>
      <name>New York University</name>
      <address>
        <city>New York</city>
        <state>New York</state>
        <zip>10012</zip>
        <country>United States</country>
      </address>
    </facility>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <verification_date>July 2020</verification_date>
  <study_first_submitted>June 15, 2020</study_first_submitted>
  <study_first_submitted_qc>July 13, 2020</study_first_submitted_qc>
  <study_first_posted type="Actual">July 17, 2020</study_first_posted>
  <last_update_submitted>July 13, 2020</last_update_submitted>
  <last_update_submitted_qc>July 13, 2020</last_update_submitted_qc>
  <last_update_posted type="Actual">July 17, 2020</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>speech</keyword>
  <keyword>articulation</keyword>
  <keyword>motor development</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Speech Sound Disorder</mesh_term>
  </condition_browse>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>

