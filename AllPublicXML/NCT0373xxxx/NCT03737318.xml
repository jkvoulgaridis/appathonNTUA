<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on July 28, 2020</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT03737318</url>
  </required_header>
  <id_info>
    <org_study_id>C-RESULTS-RCT</org_study_id>
    <nct_id>NCT03737318</nct_id>
  </id_info>
  <brief_title>Correcting Residual Errors With Spectral, Ultrasound, Traditional Speech Therapy</brief_title>
  <acronym>C-RESULTS</acronym>
  <official_title>Correcting Residual Errors With Spectral, Ultrasound, and Traditional Speech Therapy: A Randomized Controlled Trial</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>New York University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>Syracuse University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
    <collaborator>
      <agency>Montclair State University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
  </sponsors>
  <source>New York University</source>
  <oversight_info>
    <has_dmc>Yes</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      Children with speech sound disorder show diminished accuracy and intelligibility in spoken
      communication and may thus be perceived as less capable or intelligent than peers, with
      negative consequences for both socioemotional and socioeconomic outcomes. While most speech
      errors resolve by the late school-age years, between 2-5% of speakers exhibit residual speech
      errors (RSE) that persist through adolescence or even adulthood, reflecting about 6 million
      cases in the US. Both affected children/families and speech-language pathologists (SLPs) have
      highlighted the critical need for research to identify more effective forms of treatment for
      children with RSE. In a series of single-case experimental studies, research has found that
      treatment incorporating technologically enhanced sensory feedback (visual-acoustic
      biofeedback, ultrasound biofeedback) can improve speech in individuals with RSE who have not
      responded to previous intervention. A randomized controlled trial (RCT) comparing traditional
      vs biofeedback-enhanced intervention is the essential next step to inform evidence-based
      decision-making for this prevalent population. Larger-scale research is also needed to
      understand heterogeneity across individuals in the magnitude of response to biofeedback
      treatment.

      The overall objective of this proposal is to conduct clinical research that will guide the
      evidence-based management of RSE while also providing novel insights into the sensorimotor
      underpinnings of speech. The central hypothesis is that biofeedback will yield greater gains
      in speech accuracy than traditional treatment, and that individual deficit profiles will
      predict relative response to visual-acoustic vs ultrasound biofeedback. This study will
      enroll n = 118 children who misarticulate the /r/ sound, the most common type of RSE. This
      first component of the study will evaluate the efficacy of biofeedback relative to
      traditional treatment in a well-powered randomized controlled trial. Ultrasound and
      visual-acoustic biofeedback, which have similar evidence bases, will be represented equally.
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Randomized Trial Component: Previous findings suggest that biofeedback interventions can
      outperform traditional speech therapy for children with RSE, but the research base to date is
      limited to small-scale studies that do not reach the level of evidence needed to support
      large-scale changes in practice. The primary objective of the C-RESULTS RCT is to test the
      working hypothesis that a group of individuals randomly assigned to receive
      biofeedback-enhanced treatment will show larger and/or faster gains in /r/ production
      accuracy than an equivalent group receiving the same dose of non-biofeedback treatment. To
      test this hypothesis, n=110 children will be randomly assigned to receive a standard course
      of intervention with or without biofeedback. Acoustic and perceptual measures will be used to
      test for differences in both short-term learning of treated targets (Acquisition) and
      longer-term carryover of learning to untreated contexts (Generalization). In addition, a
      survey assessing participants' socio-emotional well-being will be collected from caregivers
      both pre and post treatment.
    </textblock>
  </detailed_description>
  <overall_status>Recruiting</overall_status>
  <start_date type="Actual">March 1, 2019</start_date>
  <completion_date type="Anticipated">February 1, 2024</completion_date>
  <primary_completion_date type="Anticipated">February 1, 2023</primary_completion_date>
  <phase>Phase 2</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>Randomized</allocation>
    <intervention_model>Parallel Assignment</intervention_model>
    <intervention_model_description>All participants will complete a Dynamic Assessment session (Phase 0) consisting of 2 hours of traditional (non-biofeedback) instruction. Participants will be categorized into high, moderate, and low response groups based on performance in Phase 0, and the response groups will be block randomized to traditional or biofeedback speech treatment. Within the biofeedback condition, individuals will be sub-randomized in equal numbers to receive visual-acoustic or ultrasound treatment. Participants will then complete two phases of speech treatment in their randomly assigned condition. Phase 1 (Acquisition) will consist of high-intensity, highly interactive practice delivered in three 90-minute sessions over one week. Phase 2 (Generalization) will elicit structured practice of /r/ in 16 semiweekly 45-minute sessions over 8 weeks.</intervention_model_description>
    <primary_purpose>Treatment</primary_purpose>
    <masking>Single (Outcomes Assessor)</masking>
    <masking_description>All perceptual ratings will be obtained from blinded, naive listeners recruited through online crowdsourcing. Following protocols refined in previous published research, binary rating responses will be aggregated over at least 9 unique listeners per token.</masking_description>
  </study_design_info>
  <primary_outcome>
    <measure>F3-F2 (Hz), an acoustic measure known to correlate with expert listeners' perceptual judgments of accuracy of /r/ sounds, measured from /r/ sounds produced in syllables or words during practice.</measure>
    <time_frame>through Phase I, which consists of three 90-min treatment sessions delivered over the course of approximately one week</time_frame>
    <description>Our custom Challenge-R software will present one randomly selected trial in each block of 10 with a preceding pure tone, cueing the clinician to avoid talking over the child. The stretches of the acoustic record thus flagged will be automatically annotated via forced alignment, and the first three formants (F1, F2, F3) will be extracted from a 14-msec hamming window surrounding the center of the /r/ interval. We will use the distance between the second and third formants (F3-F2) as our primary acoustic measure based on previous research showing strong agreement with expert listeners' perceptual ratings.</description>
  </primary_outcome>
  <secondary_outcome>
    <measure>Proportion of &quot;correct&quot; (vs &quot;incorrect&quot;) ratings by blinded naive listeners, a measure of perceptually rated accuracy of /r/ production, for /r/ sounds produced in word probes.</measure>
    <time_frame>Before the initiation of treatment and again after the end of all treatment (10 weeks later)</time_frame>
    <description>To assess generalization of treatment gains to untreated words, participants will read a 50-word probe and a 10-sentence probe list eliciting /r/ in various phonetic contexts. Stimuli in each probe will be presented individually in randomized order. No auditory models will be provided; for children with reading difficulty, semantic cues will be provided to elicit the intended word. Individual words will be isolated from the audio record of each word probe and presented in randomized order for binary rating (correct/incorrect) by naive listeners who are blind to treatment condition and time point (but will see the written representation of each target word). We will use the proportion of &quot;correct&quot; ratings for each token as our primary measure of perceptually rated accuracy.</description>
  </secondary_outcome>
  <secondary_outcome>
    <measure>Survey evaluating impacts of speech disorder on participants' social, emotional, and academic well-being.</measure>
    <time_frame>Before the initiation of treatment and again after the end of all treatment (10 weeks later)</time_frame>
    <description>This survey asks parents to report the impact of speech disorder on their child's social, emotional, and academic well-being. Parents are asked to circle a number from 1 to 5 (1 = Strongly disagree, 3 = Neutral, 5 = Strongly agree). For all questions, a higher score indicates a greater degree of negative impact of speech disorder on social, emotional, or academic well-being. An impact score will be calculated as described in a previous published study (Hitchcock, Harel, &amp; McAllister Byun, 2015).</description>
  </secondary_outcome>
  <number_of_arms>3</number_of_arms>
  <enrollment type="Anticipated">110</enrollment>
  <condition>Speech Sound Disorder</condition>
  <arm_group>
    <arm_group_label>Group 1</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Traditional articulation treatment</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Group 2</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Biofeedback--visual-acoustic</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Group 3</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Biofeedback-ultrasound</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Biofeedback-ultrasound</intervention_name>
    <description>In ultrasound biofeedback, the elements of traditional treatment (auditory models and verbal descriptions of articulator placement) are enhanced with a real-time ultrasound display of the shape and movements of the tongue. One or two target tongue shapes will be selected for each participant, and a trace of the selected target will be superimposed over the ultrasound screen. Participants will be cued to reshape the tongue to match this target during /r/ production.</description>
    <arm_group_label>Group 3</arm_group_label>
  </intervention>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Biofeedback--visual-acoustic</intervention_name>
    <description>In visual-acoustic biofeedback treatment, the elements of traditional treatment (auditory models and verbal descriptions of articulator placement) are enhanced with a dynamic display of the speech signal in the form of the real-time LPC (Linear Predictive Coding) spectrum. Because correct vs incorrect productions of /r/ contrast acoustically in the frequency of the third formant (F3), participants will be cued to make their real-time LPC spectrum match a visual target characterized by a low F3 frequency. They will be encouraged to attend to the visual display while adjusting the placement of their articulators and observing how those adjustments impact F3.</description>
    <arm_group_label>Group 2</arm_group_label>
  </intervention>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Traditional articulation treatment</intervention_name>
    <description>Traditional articulation treatment involves providing auditory models and verbal descriptions of correct articulator placement, then cueing repetitive motor practice. Images and diagrams of the vocal tract will be used as visual aids; however, no real-time visual display of articulatory or acoustic information will be made available.</description>
    <arm_group_label>Group 1</arm_group_label>
    <arm_group_label>Group 2</arm_group_label>
    <arm_group_label>Group 3</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:

          -  Must be between 9;0 and 15;11 years of age at the time of enrollment.

          -  Must speak English as the dominant language (i.e., must have begun learning English by
             age 2, per parent report).

          -  Must speak a rhotic dialect of English.

          -  Must pass a pure-tone hearing screening at 20 decibels Hearing Level (HL).

          -  Must pass a brief examination of oral structure and function.

          -  Must exhibit less than thirty percent accuracy, based on trained listener ratings, on
             a probe list eliciting /r/ in various phonetic contexts at the word level.

        Exclusion Criteria:

          -  Must not receive a T score more than 1.3 standard deviations (SD) below the mean on
             the Wechsler Abbreviated Scale of Intelligence-2 (WASI-2) Matrix Reasoning.

          -  Must not receive a standard score below 80 on the Core Language Index of the Clinical
             Evaluation of Language Fundamentals-5 (CELF-5).

          -  Must not exhibit voice or fluency disorder of a severity judged likely to interfere
             with the ability to participate in study activities.

          -  Must not have an existing diagnosis of developmental disability or major
             neurobehavioral syndrome such as cerebral palsy, Down Syndrome, or Autism Spectrum
             Disorder, or major neural disorder (e.g., epilepsy, agenesis of the corpus callosum)
             or insult (e.g., traumatic brain injury, stroke, or tumor resection).

          -  Must not show clinically significant signs of apraxia of speech or dysarthria.
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>9 Years</minimum_age>
    <maximum_age>15 Years</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <overall_contact>
    <last_name>Tara McAllister, PhD</last_name>
    <phone>212-992-9445</phone>
    <email>tkm214@nyu.edu</email>
  </overall_contact>
  <overall_contact_backup>
    <last_name>Twylah Campbell, MS</last_name>
    <phone>516-265-5389</phone>
    <email>tjc10@nyu.edu</email>
  </overall_contact_backup>
  <location>
    <facility>
      <name>Montclair State University</name>
      <address>
        <city>Bloomfield</city>
        <state>New Jersey</state>
        <zip>07003</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Elaine R Hitchcock, PhD</last_name>
      <phone>973-229-3797</phone>
      <email>hitchcocke@montclair.edu</email>
    </contact>
    <contact_backup>
      <last_name>Michelle T Swartz, MS</last_name>
      <phone>(610) 506-4715</phone>
      <email>turnerm5@montclair.edu</email>
    </contact_backup>
  </location>
  <location>
    <facility>
      <name>Syracuse University</name>
      <address>
        <city>Syracuse</city>
        <state>New York</state>
        <zip>13244</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Jonathan L Preston, PhD</last_name>
      <phone>315-443-3143</phone>
      <email>jopresto@syr.edu</email>
    </contact>
    <contact_backup>
      <last_name>Megan Leece, MS</last_name>
      <phone>315-443-1351</phone>
      <email>mcleece@syr.edu</email>
    </contact_backup>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <reference>
    <citation>Preston JL, McCabe P, Tiede M, Whalen DH. Tongue shapes for rhotics in school-age children with and without residual speech errors. Clin Linguist Phon. 2019;33(4):334-348. doi: 10.1080/02699206.2018.1517190. Epub 2018 Sep 10.</citation>
    <PMID>30199271</PMID>
  </reference>
  <reference>
    <citation>Preston JL, McAllister T, Phillips E, Boyce S, Tiede M, Kim JS, Whalen DH. Treatment for Residual Rhotic Errors With High- and Low-Frequency Ultrasound Visual Feedback: A Single-Case Experimental Design. J Speech Lang Hear Res. 2018 Aug 8;61(8):1875-1892. doi: 10.1044/2018_JSLHR-S-17-0441.</citation>
    <PMID>30073249</PMID>
  </reference>
  <reference>
    <citation>Dugan SH, Silbert N, McAllister T, Preston JL, Sotto C, Boyce SE. Modelling category goodness judgments in children with residual sound errors. Clin Linguist Phon. 2019;33(4):295-315. doi: 10.1080/02699206.2018.1477834. Epub 2018 May 24.</citation>
    <PMID>29792525</PMID>
  </reference>
  <reference>
    <citation>Preston JL, Holliman-Lopez G, Leece MC. Do Participants Report Any Undesired Effects in Ultrasound Speech Therapy? Am J Speech Lang Pathol. 2018 May 3;27(2):813-818. doi: 10.1044/2017_AJSLP-17-0121.</citation>
    <PMID>29546269</PMID>
  </reference>
  <reference>
    <citation>Preston JL, McAllister Byun T, Boyce SE, Hamilton S, Tiede M, Phillips E, Rivera-Campos A, Whalen DH. Ultrasound Images of the Tongue: A Tutorial for Assessment and Remediation of Speech Sound Errors. J Vis Exp. 2017 Jan 3;(119). doi: 10.3791/55123.</citation>
    <PMID>28117824</PMID>
  </reference>
  <reference>
    <citation>Preston JL, Leece MC, Maas E. Motor-based treatment with and without ultrasound feedback for residual speech-sound errors. Int J Lang Commun Disord. 2017 Jan;52(1):80-94. doi: 10.1111/1460-6984.12259. Epub 2016 Jun 14.</citation>
    <PMID>27296780</PMID>
  </reference>
  <reference>
    <citation>Campbell H, Harel D, Hitchcock E, McAllister Byun T. Selecting an acoustic correlate for automated measurement of American English rhotic production in children. Int J Speech Lang Pathol. 2018 Nov;20(6):635-643. doi: 10.1080/17549507.2017.1359334. Epub 2017 Aug 10.</citation>
    <PMID>28795872</PMID>
  </reference>
  <reference>
    <citation>Campbell H, McAllister Byun T. Deriving individualised /r/ targets from the acoustics of children's non-rhotic vowels. Clin Linguist Phon. 2018;32(1):70-87. doi: 10.1080/02699206.2017.1330898. Epub 2017 Jul 13.</citation>
    <PMID>28703653</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T. Efficacy of Visual-Acoustic Biofeedback Intervention for Residual Rhotic Errors: A Single-Subject Randomization Study. J Speech Lang Hear Res. 2017 May 24;60(5):1175-1193. doi: 10.1044/2016_JSLHR-S-16-0038.</citation>
    <PMID>28389677</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Tiede M. Perception-production relations in later development of American English rhotics. PLoS One. 2017 Feb 16;12(2):e0172022. doi: 10.1371/journal.pone.0172022. eCollection 2017.</citation>
    <PMID>28207800</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Campbell H. Differential Effects of Visual-Acoustic Biofeedback Intervention for Residual Speech Errors. Front Hum Neurosci. 2016 Nov 11;10:567. eCollection 2016.</citation>
    <PMID>27891084</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Halpin PF, Szeredi D. Online crowdsourcing for efficient rating of speech: a validation study. J Commun Disord. 2015 Jan-Feb;53:70-83. doi: 10.1016/j.jcomdis.2014.11.003. Epub 2014 Dec 15.</citation>
    <PMID>25578293</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Byun TM, Swartz M, Lazarus R. Efficacy of Electropalatography for Treating Misarticulation of /r/. Am J Speech Lang Pathol. 2017 Nov 8;26(4):1141-1158. doi: 10.1044/2017_AJSLP-16-0122.</citation>
    <PMID>28834534</PMID>
  </reference>
  <reference>
    <citation>Harel D, Hitchcock ER, Szeredi D, Ortiz J, McAllister Byun T. Finding the experts in the crowd: Validity and reliability of crowdsourced measures of children's gradient speech contrasts. Clin Linguist Phon. 2017;31(1):104-117. Epub 2016 Jun 7.</citation>
    <PMID>27267258</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Harel D, Byun TM. Social, Emotional, and Academic Impact of Residual Speech Errors in School-Aged Children: A Survey Study. Semin Speech Lang. 2015 Nov;36(4):283-94. doi: 10.1055/s-0035-1562911. Epub 2015 Oct 12.</citation>
    <PMID>26458203</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Byun TM. Enhancing generalisation in biofeedback intervention using the challenge point framework: a case study. Clin Linguist Phon. 2015 Jan;29(1):59-75. doi: 10.3109/02699206.2014.956232. Epub 2014 Sep 12.</citation>
    <PMID>25216375</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER, Swartz MT. Retroflex versus bunched in treatment for rhotic misarticulation: evidence from ultrasound biofeedback intervention. J Speech Lang Hear Res. 2014 Dec;57(6):2116-30. doi: 10.1044/2014_JSLHR-S-14-0034.</citation>
    <PMID>25088034</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER. Investigating the use of traditional and spectral biofeedback approaches to intervention for /r/ misarticulation. Am J Speech Lang Pathol. 2012 Aug;21(3):207-21. doi: 10.1044/1058-0360(2012/11-0083). Epub 2012 Mar 21.</citation>
    <PMID>22442281</PMID>
  </reference>
  <verification_date>March 2020</verification_date>
  <study_first_submitted>October 31, 2018</study_first_submitted>
  <study_first_submitted_qc>November 7, 2018</study_first_submitted_qc>
  <study_first_posted type="Actual">November 9, 2018</study_first_posted>
  <last_update_submitted>March 1, 2020</last_update_submitted>
  <last_update_submitted_qc>March 1, 2020</last_update_submitted_qc>
  <last_update_posted type="Actual">March 3, 2020</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>speech, articulation, motor development</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Communication Disorders</mesh_term>
    <mesh_term>Speech Sound Disorder</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>No</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>

