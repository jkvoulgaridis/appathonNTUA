<clinical_study>
  <!-- This xml conforms to an XML Schema at:
    https://clinicaltrials.gov/ct2/html/images/info/public.xsd -->
  <required_header>
    <download_date>ClinicalTrials.gov processed this data on July 28, 2020</download_date>
    <link_text>Link to the current ClinicalTrials.gov record.</link_text>
    <url>https://clinicaltrials.gov/show/NCT03736213</url>
  </required_header>
  <id_info>
    <org_study_id>C-RESULTS-SCED</org_study_id>
    <nct_id>NCT03736213</nct_id>
  </id_info>
  <brief_title>Delineation of Sensorimotor Subtypes Underlying Residual Speech Errors</brief_title>
  <acronym>C-RESULTS</acronym>
  <official_title>Correcting Residual Errors With Spectral, Ultrasound, and Traditional Speech Therapy: Delineation of Sensorimotor Subtypes</official_title>
  <sponsors>
    <lead_sponsor>
      <agency>New York University</agency>
      <agency_class>Other</agency_class>
    </lead_sponsor>
    <collaborator>
      <agency>Syracuse University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
    <collaborator>
      <agency>Montclair State University</agency>
      <agency_class>Other</agency_class>
    </collaborator>
  </sponsors>
  <source>New York University</source>
  <oversight_info>
    <has_dmc>Yes</has_dmc>
    <is_fda_regulated_drug>No</is_fda_regulated_drug>
    <is_fda_regulated_device>No</is_fda_regulated_device>
  </oversight_info>
  <brief_summary>
    <textblock>
      Children with speech sound disorder show diminished accuracy and intelligibility in spoken
      communication and may thus be perceived as less capable or intelligent than peers, with
      negative consequences for both socioemotional and socioeconomic outcomes. While most speech
      errors resolve by the late school-age years, between 2-5% of speakers exhibit residual speech
      errors (RSE) that persist through adolescence or even adulthood, reflecting about 6 million
      cases in the US. Both affected children/families and speech-language pathologists (SLPs) have
      highlighted the critical need for research to identify more effective forms of treatment for
      children with RSE. In a series of single-case experimental studies, research has found that
      treatment incorporating technologically enhanced sensory feedback (visual-acoustic
      biofeedback, ultrasound biofeedback) can improve speech in individuals with RSE who have not
      responded to previous intervention. Further research is needed to understand heterogeneity
      across individuals in the magnitude of response to biofeedback treatment.

      The overall objective of this proposal is to conduct clinical research that will guide the
      evidence-based management of RSE while also providing novel insights into the sensorimotor
      underpinnings of speech. The central hypothesis is that individual deficit profiles will
      predict relative response to visual-acoustic vs ultrasound biofeedback. From the larger
      population of children with RSE evaluated as part of C-RESULTS-RCT (Correcting Residual
      Errors With Spectral, Ultrasound, Traditional Speech Therapy Randomized Controlled Trial), a
      subset of 8 children will be selected who show a deficit in one domain (auditory or
      somatosensory) and intact perception in the other. Single-case methods will be used to test
      the hypothesis that sensory deficit profiles differentially predict response to
      visual-acoustic vs ultrasound biofeedback.
    </textblock>
  </brief_summary>
  <detailed_description>
    <textblock>
      Single-Case Randomization Component: At the group level, speakers with RSE show poorer
      auditory and oral somatosensory acuity than typically developing (TD) speakers, but
      individuals differ in the extent to which each sensory domain is impacted. The objective of
      this aim is to evaluate how distinct sensory profiles mediate relative response to different
      types of biofeedback, with the goal of optimizing treatment through personalized learning.
      This study will test the working hypothesis that visual-acoustic biofeedback will produce
      larger gains in children whose deficit primarily affects the specification of the auditory
      target, while ultrasound biofeedback will produce larger gains in children with a primary
      somatosensory deficit. The main approach to testing this hypothesis is to select, from the
      larger population of children with RSE evaluated as part of C-RESULTS-RCT, a subset of 8
      children who show asymmetric sensory profiles (strong auditory and weak somatosensory acuity,
      or vice versa). These children will be enrolled in a single-case experimental design where
      individual treatment sessions are randomly assigned to feature visual-acoustic or ultrasound
      biofeedback. Participants will complete 20 hrs of treatment (10 days, 2 sessions per day)
      over a 5 week period. Acoustic measures will be used to evaluate /r/ production accuracy
      within each session. Randomization tests will be used to evaluate differences in accuracy
      between ultrasound and visual-acoustic biofeedback treatment conditions.
    </textblock>
  </detailed_description>
  <overall_status>Recruiting</overall_status>
  <start_date type="Actual">March 1, 2019</start_date>
  <completion_date type="Anticipated">February 1, 2024</completion_date>
  <primary_completion_date type="Anticipated">February 1, 2023</primary_completion_date>
  <phase>Phase 1</phase>
  <study_type>Interventional</study_type>
  <has_expanded_access>No</has_expanded_access>
  <study_design_info>
    <allocation>Randomized</allocation>
    <intervention_model>Factorial Assignment</intervention_model>
    <intervention_model_description>In this single-case randomization design, each participant will receive an equal number of sessions of visual-acoustic and ultrasound biofeedback, with randomized allocation of treatment types to individual sessions. Treatment will last 20 hrs (10 days) over a 5 week period. Randomization will be blocked, with each day of treatment serving as a block; within each day, one hour of treatment will be randomly assigned to feature visual-acoustic and one to feature ultrasound treatment. A congruent condition and an incongruent condition will be defined for each participant based on their sensory profile, where the congruent condition is the biofeedback type that is expected to be more effective and the incongruent condition is the type expected to be less effective. Visual-acoustic biofeedback is defined as the congruent condition for individuals with a primary auditory deficit and ultrasound biofeedback is defined as congruent for individuals with a primary somatosensory deficit.</intervention_model_description>
    <primary_purpose>Treatment</primary_purpose>
    <masking>Single (Outcomes Assessor)</masking>
    <masking_description>All perceptual ratings will be obtained from blinded, naive listeners recruited through online crowdsourcing. Following protocols refined in previous published research, binary rating responses will be aggregated over at least 9 unique listeners per token.</masking_description>
  </study_design_info>
  <primary_outcome>
    <measure>F3-F2 (Hz), an acoustic measure known to correlate with expert listeners' perceptual judgments of accuracy of /r/ sounds, measured from /r/ sounds produced in syllables or words during practice.</measure>
    <time_frame>through Phase I, which consists of ten 90-min treatment sessions delivered over the course of approximately 5 weeks</time_frame>
    <description>The investigators' custom Challenge-R software will present one randomly selected trial in each block of 10 with a preceding pure tone, cueing the clinician to avoid talking over the child. The stretches of the acoustic record thus flagged will be automatically annotated via forced alignment, and the first three formants (F1, F2, F3) will be extracted from a 14-msec hamming window surrounding the center of the /r/ interval. The distance between the second and third formants (F3-F2) will be used as the primary acoustic measure based on previous research showing strong agreement with expert listeners' perceptual ratings.</description>
  </primary_outcome>
  <secondary_outcome>
    <measure>Survey evaluating impacts of speech disorder on participants' social, emotional, and academic well-being.</measure>
    <time_frame>Before the initiation of treatment and again after the end of all treatment (5 weeks later)</time_frame>
    <description>This survey asks parents to report the impact of speech disorder on their child's social, emotional, and academic well-being. Parents are asked to circle a number from 1 to 5 (1 = Strongly disagree, 3 = Neutral, 5 = Strongly agree). For all questions, a higher score indicates a greater degree of negative impact of speech disorder on social, emotional, or academic well-being. An impact score will be calculated as described in a previous published study (Hitchcock, Harel, &amp; McAllister Byun, 2015).</description>
  </secondary_outcome>
  <number_of_arms>2</number_of_arms>
  <enrollment type="Anticipated">8</enrollment>
  <condition>Speech Sound Disorder</condition>
  <arm_group>
    <arm_group_label>Condition 1: Visual-acoustic biofeedback</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Behavioral: Biofeedback--visual-acoustic</description>
  </arm_group>
  <arm_group>
    <arm_group_label>Condition 2: Ultrasound biofeedback</arm_group_label>
    <arm_group_type>Experimental</arm_group_type>
    <description>Behavioral: Biofeedback-ultrasound</description>
  </arm_group>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Biofeedback--visual-acoustic</intervention_name>
    <description>In visual-acoustic biofeedback treatment, the elements of traditional articulatory treatment (i.e., auditory models and verbal descriptions of articulator placement) are enhanced with a dynamic display of the speech signal in the form of the real-time LPC (Linear Predictive Coding) spectrum. Because correct vs incorrect productions of /r/ contrast acoustically in the frequency of the third formant (F3), participants will be cued to make their real-time LPC spectrum match a visual target characterized by a low F3 frequency. They will be encouraged to attend to the visual display while adjusting the placement of their articulators and observing how those adjustments impact F3.</description>
    <arm_group_label>Condition 1: Visual-acoustic biofeedback</arm_group_label>
    <arm_group_label>Condition 2: Ultrasound biofeedback</arm_group_label>
  </intervention>
  <intervention>
    <intervention_type>Behavioral</intervention_type>
    <intervention_name>Biofeedback-ultrasound</intervention_name>
    <description>In ultrasound biofeedback, the elements of traditional articulatory treatment (i.e., auditory models and verbal descriptions of articulator placement) are enhanced with a real-time ultrasound display of the shape and movements of the tongue. One or two target tongue shapes will be selected for each participant, and a trace of the selected target will be superimposed over the ultrasound screen. Participants will be cued to reshape the tongue to match this target during /r/ production.</description>
    <arm_group_label>Condition 1: Visual-acoustic biofeedback</arm_group_label>
    <arm_group_label>Condition 2: Ultrasound biofeedback</arm_group_label>
  </intervention>
  <eligibility>
    <criteria>
      <textblock>
        Inclusion Criteria:

          -  Must be between 9;0 and 15;11 years of age at the time of enrollment.

          -  Must speak English as the dominant language (i.e., must have begun learning English by
             age 2, per parent report).

          -  Must speak a rhotic dialect of English.

          -  Must pass a pure-tone hearing screening at 20 decibels (dB) Hearing Level (HL).

          -  Must pass a brief examination of oral structure and function.

          -  Must exhibit less than thirty percent accuracy, based on trained listener ratings, on
             a probe list eliciting /r/ in various phonetic contexts at the word level.

          -  Must show 0-5% accuracy in production of /r/ at the syllable level, based on treating
             clinicians' perceptual ratings, during an initial Dynamic Assessment phase consisting
             of 2 hours of traditional (non-biofeedback) instruction.

          -  Must fit one of two profiles: (1) primary auditory deficit (scores outside the
             normative predictive interval for auditory measures assessing identification and
             discrimination of synthetic speech stimuli, but within the normative predictive
             interval for measures of oral stereognosis and articulator placement awareness or (2)
             primary somatosensory deficit, with the reverse profile of spared/impaired sensory
             function.

        Exclusion Criteria:

          -  Must not receive a T score more than 1.3 standard deviations (SD) below the mean on
             the Wechsler Abbreviated Scale of Intelligence-2 (WASI-2) Matrix Reasoning.

          -  Must not receive a standard score below 80 on the Core Language Index of the Clinical
             Evaluation of Language Fundamentals-5 (CELF-5).

          -  Must not exhibit voice or fluency disorder of a severity judged likely to interfere
             with the ability to participate in study activities.

          -  Must not have an existing diagnosis of developmental disability, major neurobehavioral
             syndrome such as cerebral palsy, Down Syndrome, or Autism Spectrum Disorder, or major
             neural disorder (e.g., epilepsy, agenesis of the corpus callosum) or insult (e.g.,
             traumatic brain injury, stroke, or tumor resection).

          -  Must not show clinically significant signs of apraxia of speech or dysarthria.
      </textblock>
    </criteria>
    <gender>All</gender>
    <minimum_age>9 Years</minimum_age>
    <maximum_age>15 Years</maximum_age>
    <healthy_volunteers>Accepts Healthy Volunteers</healthy_volunteers>
  </eligibility>
  <location>
    <facility>
      <name>Montclair State University</name>
      <address>
        <city>Bloomfield</city>
        <state>New Jersey</state>
        <zip>07003</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Elaine R Hitchcock, PhD</last_name>
      <phone>973-229-3797</phone>
      <email>hitchcocke@montclair.edu</email>
    </contact>
    <contact_backup>
      <last_name>Michelle T Swartz, MS</last_name>
      <phone>(610) 506-4715</phone>
      <email>turnerm5@montclair.edu</email>
    </contact_backup>
  </location>
  <location>
    <facility>
      <name>New York University</name>
      <address>
        <city>New York</city>
        <state>New York</state>
        <zip>10012</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Not yet recruiting</status>
    <contact>
      <last_name>Tara McAllister, PhD</last_name>
      <phone>212-992-9445</phone>
      <email>tkm214@nyu.edu</email>
    </contact>
    <contact_backup>
      <last_name>Twylah Campbell, MS</last_name>
      <phone>516-265-5389</phone>
      <email>tjc10@nyu.edu</email>
    </contact_backup>
  </location>
  <location>
    <facility>
      <name>Syracuse University</name>
      <address>
        <city>Syracuse</city>
        <state>New York</state>
        <zip>13244</zip>
        <country>United States</country>
      </address>
    </facility>
    <status>Recruiting</status>
    <contact>
      <last_name>Jonathan L Preston, PhD</last_name>
      <phone>315-443-3143</phone>
      <email>jopresto@syr.edu</email>
    </contact>
    <contact_backup>
      <last_name>Megan Leece, MS</last_name>
      <phone>315-443-1351</phone>
      <email>mcleece@syr.edu</email>
    </contact_backup>
  </location>
  <location_countries>
    <country>United States</country>
  </location_countries>
  <reference>
    <citation>Preston JL, McCabe P, Tiede M, Whalen DH. Tongue shapes for rhotics in school-age children with and without residual speech errors. Clin Linguist Phon. 2019;33(4):334-348. doi: 10.1080/02699206.2018.1517190. Epub 2018 Sep 10.</citation>
    <PMID>30199271</PMID>
  </reference>
  <reference>
    <citation>Preston JL, McAllister T, Phillips E, Boyce S, Tiede M, Kim JS, Whalen DH. Treatment for Residual Rhotic Errors With High- and Low-Frequency Ultrasound Visual Feedback: A Single-Case Experimental Design. J Speech Lang Hear Res. 2018 Aug 8;61(8):1875-1892. doi: 10.1044/2018_JSLHR-S-17-0441.</citation>
    <PMID>30073249</PMID>
  </reference>
  <reference>
    <citation>Dugan SH, Silbert N, McAllister T, Preston JL, Sotto C, Boyce SE. Modelling category goodness judgments in children with residual sound errors. Clin Linguist Phon. 2019;33(4):295-315. doi: 10.1080/02699206.2018.1477834. Epub 2018 May 24.</citation>
    <PMID>29792525</PMID>
  </reference>
  <reference>
    <citation>Preston JL, Holliman-Lopez G, Leece MC. Do Participants Report Any Undesired Effects in Ultrasound Speech Therapy? Am J Speech Lang Pathol. 2018 May 3;27(2):813-818. doi: 10.1044/2017_AJSLP-17-0121.</citation>
    <PMID>29546269</PMID>
  </reference>
  <reference>
    <citation>Preston JL, McAllister Byun T, Boyce SE, Hamilton S, Tiede M, Phillips E, Rivera-Campos A, Whalen DH. Ultrasound Images of the Tongue: A Tutorial for Assessment and Remediation of Speech Sound Errors. J Vis Exp. 2017 Jan 3;(119). doi: 10.3791/55123.</citation>
    <PMID>28117824</PMID>
  </reference>
  <reference>
    <citation>Preston JL, Leece MC, Maas E. Motor-based treatment with and without ultrasound feedback for residual speech-sound errors. Int J Lang Commun Disord. 2017 Jan;52(1):80-94. doi: 10.1111/1460-6984.12259. Epub 2016 Jun 14.</citation>
    <PMID>27296780</PMID>
  </reference>
  <reference>
    <citation>Campbell H, Harel D, Hitchcock E, McAllister Byun T. Selecting an acoustic correlate for automated measurement of American English rhotic production in children. Int J Speech Lang Pathol. 2018 Nov;20(6):635-643. doi: 10.1080/17549507.2017.1359334. Epub 2017 Aug 10.</citation>
    <PMID>28795872</PMID>
  </reference>
  <reference>
    <citation>Campbell H, McAllister Byun T. Deriving individualised /r/ targets from the acoustics of children's non-rhotic vowels. Clin Linguist Phon. 2018;32(1):70-87. doi: 10.1080/02699206.2017.1330898. Epub 2017 Jul 13.</citation>
    <PMID>28703653</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T. Efficacy of Visual-Acoustic Biofeedback Intervention for Residual Rhotic Errors: A Single-Subject Randomization Study. J Speech Lang Hear Res. 2017 May 24;60(5):1175-1193. doi: 10.1044/2016_JSLHR-S-16-0038.</citation>
    <PMID>28389677</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Tiede M. Perception-production relations in later development of American English rhotics. PLoS One. 2017 Feb 16;12(2):e0172022. doi: 10.1371/journal.pone.0172022. eCollection 2017.</citation>
    <PMID>28207800</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Campbell H. Differential Effects of Visual-Acoustic Biofeedback Intervention for Residual Speech Errors. Front Hum Neurosci. 2016 Nov 11;10:567. eCollection 2016.</citation>
    <PMID>27891084</PMID>
  </reference>
  <reference>
    <citation>McAllister Byun T, Halpin PF, Szeredi D. Online crowdsourcing for efficient rating of speech: a validation study. J Commun Disord. 2015 Jan-Feb;53:70-83. doi: 10.1016/j.jcomdis.2014.11.003. Epub 2014 Dec 15.</citation>
    <PMID>25578293</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Byun TM, Swartz M, Lazarus R. Efficacy of Electropalatography for Treating Misarticulation of /r/. Am J Speech Lang Pathol. 2017 Nov 8;26(4):1141-1158. doi: 10.1044/2017_AJSLP-16-0122.</citation>
    <PMID>28834534</PMID>
  </reference>
  <reference>
    <citation>Harel D, Hitchcock ER, Szeredi D, Ortiz J, McAllister Byun T. Finding the experts in the crowd: Validity and reliability of crowdsourced measures of children's gradient speech contrasts. Clin Linguist Phon. 2017;31(1):104-117. Epub 2016 Jun 7.</citation>
    <PMID>27267258</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Harel D, Byun TM. Social, Emotional, and Academic Impact of Residual Speech Errors in School-Aged Children: A Survey Study. Semin Speech Lang. 2015 Nov;36(4):283-94. doi: 10.1055/s-0035-1562911. Epub 2015 Oct 12.</citation>
    <PMID>26458203</PMID>
  </reference>
  <reference>
    <citation>Hitchcock ER, Byun TM. Enhancing generalisation in biofeedback intervention using the challenge point framework: a case study. Clin Linguist Phon. 2015 Jan;29(1):59-75. doi: 10.3109/02699206.2014.956232. Epub 2014 Sep 12.</citation>
    <PMID>25216375</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER, Swartz MT. Retroflex versus bunched in treatment for rhotic misarticulation: evidence from ultrasound biofeedback intervention. J Speech Lang Hear Res. 2014 Dec;57(6):2116-30. doi: 10.1044/2014_JSLHR-S-14-0034.</citation>
    <PMID>25088034</PMID>
  </reference>
  <reference>
    <citation>Byun TM, Hitchcock ER. Investigating the use of traditional and spectral biofeedback approaches to intervention for /r/ misarticulation. Am J Speech Lang Pathol. 2012 Aug;21(3):207-21. doi: 10.1044/1058-0360(2012/11-0083). Epub 2012 Mar 21.</citation>
    <PMID>22442281</PMID>
  </reference>
  <verification_date>February 2020</verification_date>
  <study_first_submitted>November 7, 2018</study_first_submitted>
  <study_first_submitted_qc>November 7, 2018</study_first_submitted_qc>
  <study_first_posted type="Actual">November 9, 2018</study_first_posted>
  <last_update_submitted>March 1, 2020</last_update_submitted>
  <last_update_submitted_qc>March 1, 2020</last_update_submitted_qc>
  <last_update_posted type="Actual">March 4, 2020</last_update_posted>
  <responsible_party>
    <responsible_party_type>Sponsor</responsible_party_type>
  </responsible_party>
  <keyword>speech</keyword>
  <keyword>articulation</keyword>
  <keyword>motor development</keyword>
  <condition_browse>
    <!-- CAUTION:  The following MeSH terms are assigned with an imperfect algorithm            -->
    <mesh_term>Speech Sound Disorder</mesh_term>
  </condition_browse>
  <patient_data>
    <sharing_ipd>No</sharing_ipd>
  </patient_data>
  <!-- Results have not yet been posted for this study                                          -->
</clinical_study>

